import os
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from database import get_thread_history
from agent_state import AgentState

def response_generator(state: AgentState):
    print("\n--- ðŸ§  AGENT: HIGH-REASONING RESPONSE GENERATOR ---")
    
    # 1. Prepare Context Sources
    chat_history = get_thread_history(state['thread_id'])
    retrieved_docs = state.get('retrieved_context', [])
    
    # 2. Dynamic Context Logic
    # If retriever found nothing, we rely on the LLM's internal 2026 knowledge.
    if not retrieved_docs or len(retrieved_docs) == 0:
        context_source = "INTERNAL_REASONING"
        formatted_context = "No specific internal documents found. Use your core training and logical reasoning to answer."
    else:
        context_source = "RAG_KNOWLEDGE_BASE"
        # Format chunks with source metadata for FAANG-level citations
        formatted_context = "\n".join([
            f"[Source: {doc.metadata.get('source', 'Unknown')}]: {doc.page_content}" 
            for doc in retrieved_docs
        ])

    # 3. The Professional Multi-Domain Prompt
    system_message = f"""
    You are a Recursive Reasoning AI Assistant specializing in {state['category']}.
    Current Date: 2026. Context Mode: {context_source}.

    ### OPERATIONAL PROTOCOLS:
    - **MATH/LOGIC:** Break down problems step-by-step. Use LaTeX for equations (e.g., $$x = \frac{{-b \pm \sqrt{{d}}}}{{2a}}$$).
    - **REASONING:** Use Chain-of-Thought. State your assumptions before your conclusion.
    - **CODING:** Provide modular, secure snippets with comments explaining the 'why'.
    - **RAG USAGE:** If context is provided, prioritize it. Cite your sources using [Source Name].

    ### BEHAVIORAL CONSTRAINTS:
    - START the email directly. NO "I hope this finds you well" or "As an AI..."
    - If the answer is not in the context and you are unsure, admit it and suggest a human follow-up.
    - Keep formatting scannable: Use bolding for key terms and bullet points for steps.
    """

    user_message = f"""
    ### MEMORY (Past Interactions):
    {chat_history}

    ### KNOWLEDGE BASE (Reference Material):
    {formatted_context}

    ### INCOMING REQUEST:
    Subject: {state['subject']}
    Body: {state['raw_email']}

    ### DRAFT THE RESPONSE:
    """

    try:
        # Using Llama 3.1 70B for higher reasoning depth if possible, or 8B for speed
        llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0.3) 
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("human", user_message)
        ])
        
        chain = prompt | llm
        response = chain.invoke({})

        return {
            "draft_reply": response.content,
            "steps": [f"Generated {state['category']} response via {context_source}."]
        }

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return {"draft_reply": "System busy. Retrying...", "steps": [f"Error: {str(e)}"]}
